{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip list\n",
    "\n",
    "import numpy as np\n",
    "import local_utils\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "\n",
    "from distutils.util import strtobool\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "from utils.parse_config import *\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from re import compile as compile_re\n",
    "from glob import glob as glob_glob\n",
    "\n",
    "from collections import namedtuple\n",
    "import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "from models import *\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The eval data\n",
    "Define data and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_path = \"data/PST900_RGBT_Dataset/test\"\n",
    "test_loader = torch.utils.data.DataLoader(RGBTFolderP(test_path), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save eval data in .npz format for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_data = []\n",
    "quantization_labels = []\n",
    "\n",
    "for _, img, target in test_loader:\n",
    "    quantization_data.append(img)\n",
    "    quantization_labels.append(target)\n",
    "\n",
    "batch_shape = np.shape(quantization_data[0].numpy())\n",
    "print(batch_shape)\n",
    "\n",
    "test_X = torch.cat(quantization_data).numpy()\n",
    "test_Y = torch.cat(quantization_labels).numpy()\n",
    "\n",
    "np.savez('eval_PST900.npz', data=test_X, targets=test_Y)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Init the float model\n",
    "Create the model and load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model_config_path = \"config/yolov3.cfg\"\n",
    "weights_path = \"weights/float_133.weightd\"\n",
    "\n",
    "net = Darknet(model_config_path)\n",
    "net.load_weights_dict(weights_path)\n",
    "net.to(device)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "print(device)\n",
    "with open(\"log_net.txt\", \"w\") as f:\n",
    "    f.write(str(net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalLoader:\n",
    "    def __init__(self, \n",
    "                 batch_size: int = 1, \n",
    "                 npz_path: str = 'eval_PST900.npz') -> None:\n",
    "        data = np.load(npz_path)\n",
    "        self.data = data['data'].astype(np.float32)\n",
    "        self.targets = data['targets']\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if i >= len(self):\n",
    "            raise StopIteration\n",
    "\n",
    "        beg = min(i * self.batch_size, self.data.shape[0])\n",
    "        end = min(beg + self.batch_size, self.data.shape[0])\n",
    "\n",
    "        return self.data[beg:end, ...], self.targets[beg:end]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = EvalLoader()\n",
    "for batch_i, (imgs, targets) in enumerate(loader):\n",
    "    print(\"Batch:\", batch_i)\n",
    "    print(\"Img:\",imgs.shape)\n",
    "    print(\"Target:\",targets.shape)\n",
    "    img = imgs\n",
    "    target = targets\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Eval the float model\n",
    "Ensure the model was loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluation(model: nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               conf_thres: int = 0.5,\n",
    "               nms_thres: int = 0.4\n",
    "               ):\n",
    "\n",
    "    num_classes = 4\n",
    "    img_size = 416\n",
    "    iou_thres = 0.5\n",
    "    \n",
    "    # YOLO layers needed for evaluation, because the net was split before and does not contain the YOLO layer\n",
    "    yoloL = YOLOLayer([(116, 90), (156, 198), (373, 326)], num_classes, img_size)\n",
    "    yoloM = YOLOLayer([(30, 61), (62, 45), (59, 119)], num_classes, img_size)\n",
    "    yoloS = YOLOLayer([(10, 13), (16, 30), (33, 23)], num_classes, img_size)    \n",
    "\n",
    "    print(\"Compute mAP...\")\n",
    "\n",
    "    all_detections = []\n",
    "    all_annotations = []\n",
    "\n",
    "    for _, (imgs, targets) in enumerate(tqdm.tqdm(data_loader, desc=\"Detecting objects\")):\n",
    "\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "        targets = torch.from_numpy(targets)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(imgs)\n",
    "            outputs = []\n",
    "            y1 = yoloL(output[0])\n",
    "            y2 = yoloM(output[1])\n",
    "            y3 = yoloS(output[2])\n",
    "            outputs.append(y1)\n",
    "            outputs.append(y2)\n",
    "            outputs.append(y3)\n",
    "            outputs = torch.cat(outputs, 1)\n",
    "            outputs = non_max_suppression(outputs, 4, conf_thres=conf_thres, nms_thres=nms_thres)\n",
    "\n",
    "        for output, annotations in zip(outputs, targets):\n",
    "            all_detections.append([np.array([]) for _ in range(num_classes)])\n",
    "            if output is not None:\n",
    "                # Get predicted boxes, confidence scores and labels\n",
    "                pred_boxes = output[:, :5].cpu().numpy()\n",
    "                scores = output[:, 4].cpu().numpy()\n",
    "                pred_labels = output[:, -1].cpu().numpy()\n",
    "\n",
    "                # Order by confidence\n",
    "                sort_i = np.argsort(scores)\n",
    "                pred_labels = pred_labels[sort_i]\n",
    "                pred_boxes = pred_boxes[sort_i]\n",
    "\n",
    "                for label in range(num_classes):\n",
    "                    all_detections[-1][label] = pred_boxes[pred_labels == label]\n",
    "\n",
    "            all_annotations.append([np.array([]) for _ in range(num_classes)])\n",
    "            if any(annotations[:, -1] > 0):\n",
    "\n",
    "                annotation_labels = annotations[annotations[:, -1] > 0, 0].cpu().numpy()\n",
    "                _annotation_boxes = annotations[annotations[:, -1] > 0, 1:].cpu()\n",
    "\n",
    "                # Reformat to x1, y1, x2, y2 and rescale to image dimensions\n",
    "                annotation_boxes = np.empty_like(_annotation_boxes)\n",
    "                annotation_boxes[:, 0] = _annotation_boxes[:, 0] - _annotation_boxes[:, 2] / 2\n",
    "                annotation_boxes[:, 1] = _annotation_boxes[:, 1] - _annotation_boxes[:, 3] / 2\n",
    "                annotation_boxes[:, 2] = _annotation_boxes[:, 0] + _annotation_boxes[:, 2] / 2\n",
    "                annotation_boxes[:, 3] = _annotation_boxes[:, 1] + _annotation_boxes[:, 3] / 2\n",
    "                annotation_boxes *= img_size\n",
    "\n",
    "                for label in range(num_classes):\n",
    "                    all_annotations[-1][label] = annotation_boxes[annotation_labels == label, :]\n",
    "\n",
    "    average_precisions = {}\n",
    "    for label in range(num_classes):\n",
    "        true_positives = []\n",
    "        scores = []\n",
    "        num_annotations = 0\n",
    "\n",
    "        for i in tqdm.tqdm(range(len(all_annotations)), desc=f\"Computing AP for class '{label}'\"):\n",
    "            detections = all_detections[i][label]\n",
    "            annotations = all_annotations[i][label]\n",
    "\n",
    "            num_annotations += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for *bbox, score in detections:\n",
    "                scores.append(score)\n",
    "\n",
    "                if annotations.shape[0] == 0:\n",
    "                    true_positives.append(0)\n",
    "                    continue\n",
    "\n",
    "                overlaps = bbox_iou_numpy(np.expand_dims(bbox, axis=0), annotations)\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap = overlaps[0, assigned_annotation]\n",
    "\n",
    "                if max_overlap >= iou_thres and assigned_annotation not in detected_annotations:\n",
    "                    true_positives.append(1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    true_positives.append(0)\n",
    "\n",
    "        # no annotations -> AP for this class is 0\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "\n",
    "        true_positives = np.array(true_positives)\n",
    "        false_positives = np.ones_like(true_positives) - true_positives\n",
    "        # sort by score\n",
    "        indices = np.argsort(-np.array(scores))\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives = true_positives[indices]\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision\n",
    "        recall = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision = compute_ap(recall, precision)\n",
    "        average_precisions[label] = average_precision\n",
    "\n",
    "    logger = {}\n",
    "    print(\"Average Precisions:\")\n",
    "    for c, ap in average_precisions.items():\n",
    "        print(f\"+ Class '{c}' - AP: {ap}\")\n",
    "        logger[c] = ap\n",
    "\n",
    "    mAP = np.mean(list(average_precisions.values()))\n",
    "    logger[\"mAP\"] = mAP\n",
    "    print(f\"mAP: {mAP}\")\n",
    "\n",
    "    return logger, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(net, loader, 0.8, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.1, 1.0, 0.1)\n",
    "y = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "data = np.zeros((X.shape[0], Y.shape[1]))\n",
    "\n",
    "for conf, nms in zip(X.ravel(), Y.ravel()):\n",
    "    i = int(conf * 10 - 1)\n",
    "    j = int(nms * 10 - 1)\n",
    "    print(\"Conf:\", conf, \"NMS:\", nms)\n",
    "    _, mAP = evaluation(net, loader, conf, nms)\n",
    "    data[i, j] = mAP\n",
    "\n",
    "with open('mAP_tests.json', 'w') as f:\n",
    "    json.dump(data.tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quantize PTQ (Post Training Quantization)\n",
    "\n",
    "Stage 1: Calibration of Vitis AI Quantizer\n",
    "\n",
    "Stage 2: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             conf = 0.1,\n",
    "             nms = 0.1):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    with tm:\n",
    "        # available in docker or after packaging \n",
    "        # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "        # and installing the package\n",
    "        print(\"before import\")\n",
    "        from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "        # model to device\n",
    "        print(\"Before device\")\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "#         output = quantized_model(rand_in) # IMPORTANT! if the data is not passed through the model, the model will not be quantized\n",
    "        print(\"testing st\")\n",
    "        evaluation(quantized_model, dataloader, conf, nms)\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quantize(float_model=net, \n",
    "         input_shape=batch_shape,\n",
    "         quant_dir='quant_dir',\n",
    "         quant_mode='calib',\n",
    "         device=device,\n",
    "         dataloader=loader\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize(float_model=net, \n",
    "         input_shape=batch_shape,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         quant_mode='test',\n",
    "         device=device,\n",
    "         dataloader=loader\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "!vai_c_xir --xmodel 'quant_dir/Darknet_int.xmodel' --arch arch.json --net_name Darknet_qu --output_dir build"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
