{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55b6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af6675",
   "metadata": {},
   "source": [
    "### Simple model for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3683ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        # First convolutional layer: input channels = 1, output channels = 16, kernel size = 3\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Second convolutional layer: input channels = 16, output channels = 32, kernel size = 3\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc = nn.Linear(1568, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, followed by ReLU and max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Apply second convolution, followed by ReLU and max pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f82021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvNet()\n",
    "input_tensor = torch.randn(1, 1, 28, 28)  # Example input tensor of shape (batch_size, channels, height, width)\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814a9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    \"\"\"\n",
    "    # available in docker or after packaging \n",
    "    # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "    # and installing the package\n",
    "    print(\"before import\")\n",
    "    from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "    # model to device\n",
    "    print(\"Before device\")\n",
    "    model = float_model.to(device)\n",
    "\n",
    "    # Force to merge BN with CONV for better quantization accuracy\n",
    "    optimize = 1\n",
    "\n",
    "    rand_in = torch.randn(input_shape)\n",
    "    print(\"get qunatizer start\")\n",
    "    try:\n",
    "        quantizer = torch_quantizer(\n",
    "            quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\")\n",
    "        print(e)\n",
    "        return\n",
    "    print(\"get qunatizer end\")\n",
    "\n",
    "    print(\"get quantized model start\")\n",
    "    quantized_model = quantizer.quant_model\n",
    "    print(\"get quantized model end\")\n",
    "\n",
    "    # evaluate\n",
    "    output = quantized_model(rand_in)\n",
    "\n",
    "    # export config\n",
    "    if quant_mode == 'calib':\n",
    "        print(\"export config\")\n",
    "        quantizer.export_quant_config()\n",
    "        print(\"export config end\")\n",
    "    # export model\n",
    "    if quant_mode == 'test':\n",
    "        print(\"export xmodel\")\n",
    "        quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "        print(\"export xmodel end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize(float_model=model, \n",
    "         input_shape=(1, 1, 28, 28),\n",
    "         quant_dir='quant_dir_simple',\n",
    "         quant_mode='calib',\n",
    "         device=torch.device(\"cpu\"),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize(float_model=model, \n",
    "         input_shape=(1, 1, 28, 28),\n",
    "         quant_dir='quant_dir_simple',\n",
    "         quant_mode='test',\n",
    "         device=torch.device(\"cpu\"),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "!vai_c_xir --xmodel 'quant_dir_simple/SimpleConvNet_int.xmodel' --arch arch2.json --net_name SimpleConvNet_qu --output_dir build"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
